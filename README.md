# ğŸ“˜ Sentiment Analysis with DistilBERT  
[![HuggingFace](https://img.shields.io/badge/HuggingFace-Transformers-orange?logo=huggingface)]()
[![Kaggle](https://img.shields.io/badge/Kaggle-GPU%20Accelerated-blue?logo=kaggle)]()
[![Python](https://img.shields.io/badge/Python-3.10-green?logo=python)]()
[![Model](https://img.shields.io/badge/Model-DistilBERT-yellow)]()

A beginnerâ€‘friendly, endâ€‘toâ€‘end NLP project that fineâ€‘tunes **DistilBERT** for sentiment analysis using the **IMDB dataset**, **Kaggle GPU accelerators**, and **Hugging Face Transformers**.

---

## ğŸš€ Features
- Fineâ€‘tune DistilBERT in 3â€“5 minutes on Kaggle GPU  
- HuggingFace `datasets` + `transformers`  
- Mixed precision FP16 training  
- Clean inference pipeline  
- Optional FastAPI deployment  
- Beginnerâ€‘friendly explanations

---

## ğŸ“ Project Structure
```
repo/
â”‚â”€â”€ README.md
â”‚â”€â”€ requirements.txt
â”‚â”€â”€ app.py
â”‚â”€â”€ src/
â”‚   â””â”€â”€ inference_example.py
â”‚â”€â”€ notebook/
â”‚   â””â”€â”€ training_notebook_placeholder.txt
```

---

## ğŸ§  Model: DistilBERT
DistilBERT is 40% smaller, 60% faster, and retains 97% of BERTâ€™s accuracy.  
Ideal for learning NLP and running on free GPUs.

---

## ğŸ›  Installation
```bash
pip install -r requirements.txt
```

---

## ğŸ‹ï¸ Training (Kaggle Recommended)
Use the Kaggle notebook for:
- GPU acceleration  
- FP16 mixed precision  
- Fast dataset loading  

---

## ğŸ” Inference
```python
from transformers import pipeline

pipe = pipeline("sentiment-analysis", model="./results")
print(pipe("This movie was great!"))
```

---

## ğŸŒ FastAPI Deployment
Run:
```bash
uvicorn app:app --reload
```

---

## ğŸ“ License
MIT License
