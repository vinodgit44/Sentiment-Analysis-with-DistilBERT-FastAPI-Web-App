# ğŸ“˜ Sentiment Analysis with DistilBERT  
[![HuggingFace](https://img.shields.io/badge/HuggingFace-Transformers-orange?logo=huggingface)]()
[![Kaggle](https://img.shields.io/badge/Kaggle-GPU%20Accelerated-blue?logo=kaggle)]()
[![Python](https://img.shields.io/badge/Python-3.10-green?logo=python)]()
[![Model](https://img.shields.io/badge/Model-DistilBERT-yellow)]()

ğŸ“˜ Sentiment Analysis with DistilBERT â€” FastAPI Web App

This project demonstrates a complete end-to-end NLP pipeline using HuggingFace Transformers, DistilBERT, and FastAPI.
It includes:
âœ” Dataset loading
âœ” Tokenization
âœ” Fine-tuning DistilBERT
âœ” Saving the model
âœ” Building a modern Bootstrap UI
âœ” Deploying an API for real-time sentiment prediction


## ğŸ§  Model: DistilBERT
DistilBERT is 40% smaller, 60% faster, and retains 97% of BERTâ€™s accuracy.  
Ideal for learning NLP and running on free GPUs.

## ğŸš€ Features
- Fineâ€‘tune DistilBERT in 3â€“5 minutes on Kaggle GPU  
- HuggingFace `datasets` + `transformers`  
- Mixed precision FP16 training  
- Clean inference pipeline  
- Optional FastAPI deployment  
- Beginnerâ€‘friendly explanations

## ğŸ“¦ Installation

1ï¸âƒ£ Clone the repo
```bash
git clone https://github.com/yourusername/DistilBERT_Sentiment_Repo.git
cd DistilBERT_Sentiment_Repo
```
ğŸ“ Project Structure
```
repo/
â”‚â”€â”€ README.md
â”‚â”€â”€ requirements.txt
â”‚â”€â”€ app.py
â”‚â”€â”€ src/
â”‚   â””â”€â”€ inference_example.py
â”‚â”€â”€ notebook_train/
â”‚   â””â”€â”€ main.py
```


2ï¸âƒ£ Create virtual env
```bash
python3 -m venv .venv
source .venv/bin/activate
```


3ï¸âƒ£ Install dependencies

```python
pip install -r requirements.txt
```

---
## ğŸ‹ï¸ Training (Kaggle Recommended)
Use the Kaggle notebook for:
- GPU acceleration  
- FP16 mixed precision  
- Fast dataset loading
- Training the Model

Your dataset should look like:
train.csv / test.csv
text,label
"I love this product!",1
"This is terrible.",0

Run training:
notebook-train/main.py
```
â”‚â”€â”€ notebook_train/
â”‚   â””â”€â”€ main.py
```

What happens:
Tokenizer loads
Dataset is tokenized
DistilBERT is fine-tuned
Metrics (Accuracy, F1) are computed
Model is saved into ./results/

## ğŸ“ˆ Model Performance

After training you will see:
Epoch 1/2 â€“ Accuracy: 0.89, F1: 0.88
Epoch 2/2 â€“ Accuracy: 0.92, F1: 0.91



ğŸ›  Customization
You can modify:
Learning rate
Batch size
Number of labels
Model architecture

Or replace DistilBERT with:
BERT-base
RoBERTa
DeBERTa
ALBERT

## ğŸ” Inference
```python
from transformers import pipeline

pipe = pipeline("sentiment-analysis", model="./results")
print(pipe("This movie was great!"))
```

## ğŸŒ FastAPI Deployment
Run:
```bash
uvicorn app:app --reload
```


ğŸ¨ UI
http://127.0.0.1:8000/ui

ğŸ§ª API (JSON)
http://127.0.0.1:8000/docs

âœ¨ UI Preview (Description)
Input box for text
Bootstrap card layout
Color-coded results:
Green = Positive ğŸ™‚
Red = Negative ğŸ˜¡
Orange = Neutral ğŸ˜

ğŸ§ª Example Predictions
Text	Output
â€œI love this!â€	POSITIVE ğŸ™‚
â€œThis is the worst.â€	NEGATIVE ğŸ˜¡
â€œIt works.â€	NEUTRAL ğŸ˜


ğŸ“¤ Additional Deployment Options
You can deploy this app on:

ğŸ”¹ HuggingFace Spaces (Free)

Supports Gradio & FastAPI

ğŸ”¹ AWS EC2

Production + scaling

ğŸ”¹ Docker
```bash
docker build -t sentiment-app .
docker run -p 8000:8000 sentiment-app
```

## â¤ï¸ Credits
Built using:
HuggingFace Transformers
FastAPI
Bootstrap
PyTorch

## â­ Contribute

Pull requests welcome!
You can:
Improve UI
Add datasets
Add multi-language support
Add ONNX optimization



ğŸ“ License
MIT License
